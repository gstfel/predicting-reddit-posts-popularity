{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores machine learning methods to predict the popularity of reddit posts. Note that most explanations and interpretations are provided in the corresponding report. Our team is called TeamScholz and consists of Martin Scholz (11910748), Mark Tallai (01452456), Felix Gstettner (11810614) and Stepan Malysh (11929354). This code in this notebook starts by defining relevant functions that can be used for accessing and preprocessing the data and also for estimating models. The final section uses this functions to request the posts of r/elderscrollsonline and r/Atlanta. Next to the code chunks short explanations will be provided in markdown-cells. Note that this notebook focuses on defining the functions and running them on the corresponding subreddits. Explanations and interpretations can be found in detail in the report.\n",
    "\n",
    "By using this notebook and the attached data you agree to comply with our privacy policy provided at https://github.com/hellfun1/Privacy_policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Spark\n",
    "As a first step, all required packages are imported and a spark session is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required packages\n",
    "#praw for connecting to the Reddit API\n",
    "import praw\n",
    "#import time to get the current time\n",
    "import time\n",
    "#import sys to get error messages\n",
    "import sys\n",
    "#note: we are aware of the fact that pandas is NOT scaleable. Pandas is only used as a helper function in order to get a local backup of our requested\n",
    "#data. This was done to ensure reproducibility as the requested data depends on the exact time when the functions were called. Thus,\n",
    "#to ensure reproducibility we added the data used for our analyses as csv-files to our submission. Exept from saving the data, pandas is NOT used for our algorithms.\n",
    "import pandas\n",
    "import math\n",
    "from functools import reduce\n",
    "#for plotting silhouette score:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load machine learning tools\n",
    "# Import the StandardScaler Module to standardize feature vector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "#impport Vector Assembler to create feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import  Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "#import functions from pyspark sql to replace/remove unneccessary words\n",
    "from pyspark.sql import functions\n",
    "#import the SparkSession module for using spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, array\n",
    "from pyspark.sql.functions import translate\n",
    "from pyspark.sql.functions import concat_ws, lit\n",
    "from pyspark.sql.functions import struct    \n",
    "from pyspark.sql.types import *\n",
    "#import function to get the means\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#for clustering we use KMeans\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#import functions for models\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "from  pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Predicting Reddit Popularity\") \\\n",
    "    .config(\"spark.executor.memory\", \"2gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Main entry point for Spark functionality. A SparkContext represents the\n",
    "# connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
    "# broadcast variables on that cluster.      \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data and preprocessing\n",
    "If needed the following code chunk installs the praw library. Note that we were required to install praw every time we restarted the server. Therefore, we decided to add this code chunk if an installation is needed. Only run the next code chunk if praw is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/conda/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/conda/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /opt/conda/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from update-checker>=0.18->praw) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the preprocessing function\n",
    "The following functions start by opening a connection to the Reddit API. Next, the function requests the posts of a given subreddit. Furthermore, the functions also clean and preprocess the posts so the data can be used in the following steps. In fact, the youngest posts are omitted and several elements in the strings are removed. Rows containing missing values (for example the body would be empty if a post only contains a picture) are omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###inputs\n",
    "##subreddit: what subbreddit\n",
    "##post_limit: how many posts should be fetched maximium of 1000\n",
    "##max_age: maximum age of post in unix time units\n",
    "###output:\n",
    "##a pyspark data frame that contains our relevant data\n",
    "def accessing(subreddit, post_limit, max_age):\n",
    "    try:\n",
    "        #establish connection to Reddit via the API\n",
    "        reddit = praw.Reddit(client_id = 'client id',\n",
    "                            client_secret = 'client secret',\n",
    "                            username = 'username',\n",
    "                            password = 'password',\n",
    "                            user_agent = 'user agent')\n",
    "\n",
    "        column_names = [\"body\" ,\"unix_time\", \"num_comments\", \"score\", \"upvote_ratio\"]\n",
    "        \n",
    "        #we first create a pyspark dataframe\n",
    "        #to infer the schema, i.e. that there should be 6 columns with the correct data type\n",
    "        #we specify some strings, integers and floats for the first row\n",
    "        #the body is \"#\"\n",
    "        #this string is later on replaced by an empty sting and later on the whole row is omitted\n",
    "        #this is an in-between step that helps saving the data in a pyspark dataframe\n",
    "        data_DF = spark.createDataFrame([(\"#\",100,100,100,0.5)], column_names)\n",
    "        \n",
    "        #access the data about the posts\n",
    "        for sub in reddit.subreddit(subreddit).new(limit=post_limit):\n",
    "            new_post = spark.createDataFrame([(sub.selftext, sub.created_utc, sub.num_comments, sub.score, sub.upvote_ratio)], column_names)\n",
    "            data_DF = data_DF.union(new_post)\n",
    "\n",
    "\n",
    "        #examine schema:\n",
    "        #note that the following code line would show that there is no problem regarding the data types\n",
    "        #i.e., numeric values are represented as either floats or strings, characters are correctly saved as string\n",
    "        #data_DF.printSchema()\n",
    "        #thus, there is no need to convert data types, they were identified correctly each time we ran the code\n",
    "        #note that we tried this out with different subreddits and the schema was correctly identified in each case\n",
    "        \n",
    "        #exclude the youngest posts\n",
    "        # get the time\n",
    "        time = time.time()\n",
    "        #filter out young posts\n",
    "        data_DF = data_DF.filter(time - data_DF.unix_time>= max_age)#86400 is one day\n",
    "\n",
    "        #clean the text of the posts\n",
    "        #more cleaning will be done in the preprocessing-function later on\n",
    "        #replace missing values with \"None\"\n",
    "        data_DF = data_DF.na.replace('', None)\n",
    "        \n",
    "        #removing commas and paragraphs is especially important if the data is saved locally as a csv-file\n",
    "        #as we are providing a hard save of our used data, commas and paragraphs in the body will lead to messy results when the csv is read into\n",
    "        #a pyspark dataframe. Thus omitting them before the local save ensures that the data is stored correctly and could be used without further issues\n",
    "        #remove commas\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', ',', ''))\n",
    "        #remove paragraphs\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', '\\n', ''))\n",
    "        \n",
    "        #drop missing values\n",
    "        data_DF = data_DF.na.drop()\n",
    "        \n",
    "        return data_DF\n",
    "    \n",
    "    except:\n",
    "        #troubleshooting:\n",
    "        #check if given argument is string\n",
    "        if (isinstance(subreddit, str) == False):\n",
    "            print(\"Error: Argument must be a string.\")\n",
    "        else:\n",
    "            #get other error messages\n",
    "            #might be, e.g., that a given Subreddit does not exist\n",
    "            #which will give either a 404 HTTP error (i.e. \"not found\")\n",
    "            #or a message that the request was redirected to the search page (e.g., if the given subreddit does not exist)\n",
    "            #note that there might also occur a SparkException Error that may arise because the dataframe is empty,\n",
    "            #i.e., there are no observations left (this might occur if the time threshold for youngest posts is too big compared to the age of the oldest post)\n",
    "            print(\"Unexpected error:\", sys.exc_info()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###inputs:\n",
    "##subreddit: what subbreddit\n",
    "##post_limit: how many posts should be fetched, this is limited by Reddit to approximately 1000 posts \n",
    "##max_age: maximum age of post in unix time units\n",
    "###output:\n",
    "##a pyspark data frame that contains our relevant data\n",
    "def preprocessing(df):\n",
    "    try:\n",
    "        #data preprocessing:\n",
    "        data_DF = df\n",
    "\n",
    "        #clean posts:\n",
    "        #remove expressions based on seuggestions made in the lab sessions\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'http\\S+', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', '#', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', ':', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', ';', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'\\n', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'\\*', ''))\n",
    "        #remove mentioned user names in posts\n",
    "        #note that there are two ways on Reddit to link users, /u and /u/\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'/u/+', ''))\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'u/+', ''))\n",
    "        #remove zero-width space\n",
    "        data_DF = data_DF.withColumn('body', functions.regexp_replace('body', r'&x200B', ''))\n",
    "\n",
    "        #drop missing values\n",
    "        data_DF = data_DF.na.drop()\n",
    "\n",
    "        #collect the relevant variables for clustering in a single column called features\n",
    "        #clustering will be based on the number of comments, score and the upvote ratio\n",
    "        vecAssembler = VectorAssembler(inputCols=[\"num_comments\", \"score\", \"upvote_ratio\"], outputCol=\"features\")\n",
    "        data_DF_fea = vecAssembler.transform(data_DF)\n",
    "\n",
    "        # Standardizes features by removing the mean and scaling to unit variance using column summary\n",
    "        # statistics on the samples in the training set.\n",
    "        standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "        # Fits a model to the input dataset with optional parameters.\n",
    "        scaler = standardScaler.fit(data_DF_fea)\n",
    "\n",
    "        # Transforms the input dataset with optional parameters.\n",
    "        DF_scaled = scaler.transform(data_DF_fea)\n",
    "\n",
    "        #return the preprocessed spark dataframe\n",
    "        #and print the remaining number of observations\n",
    "        print(\"The cleaned dataframe consists of \"+ str(DF_scaled.count()) + \" observations.\")\n",
    "        return DF_scaled\n",
    "    \n",
    "    except:\n",
    "        #troubleshooting:\n",
    "        #get error messages\n",
    "        #note that there might also occur a SparkException Error that may arise because the the dataframe is empty,\n",
    "        #i.e., there are no observations left\n",
    "        print(\"Unexpected error:\", sys.exc_info()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section defines functions that are used for clustering. Clustering is based on the collected metadata (number of comments, score and upvote ratio). We are going to use the K-means method. The first function, get_K(), uses the silhouette score to find out the optimal number of clusters K. The second function, cluster(), uses the optimal number of clusters obtained by the previous function to cluster the data based on the respective metadata. Note that we restricted the possible values for K to lie between 3 and 10. The lower boundary ensures that for a given dataframe there exist some clusters. The upper boundary is limited to 10 in order to limit computational time. While a higher number of clusters might be more optimal, K-means clustering has the big disadvantage that it is time-consuming. In our case, clustering is running eight times (once for each possible value of K). The higher K, the higher the overall computational time. Therefore, K is limited to 10. The code regarding clustering was partly adapted from https://rsandstroem.github.io/sparkkmeans.html. Please note that until now we have defined a try and except statement to ensure error handling. This will be not included in any of the following functions as there should not be any problems if the correct dataframe is given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K(df):\n",
    "    # get optimal K for K means clustering\n",
    "    #output will be a plot based on silhouette score\n",
    "    #which can be used to find out the optimal number of clusters K\n",
    "    try:\n",
    "        #create evaluator\n",
    "        #we are going to use the silhouette score based on a squared euclidean distance measure\n",
    "        evaluator_sil = ClusteringEvaluator(predictionCol='prediction', featuresCol='features_scaled', metricName='silhouette', distanceMeasure='squaredEuclidean')    \n",
    "\n",
    "        #compute \"cost\" for different K\n",
    "        #we try out values from 3 to 10\n",
    "        #set seed for reproducibility\n",
    "        score = []\n",
    "        for k in range(3,10):\n",
    "            #note that this for loop is one of the most time consuming parts of the code\n",
    "            #get model for clustering\n",
    "            kmeans = KMeans().setK(k).setSeed(123).setFeaturesCol(\"features_scaled\")\n",
    "            #assign clusters to observations\n",
    "            model = kmeans.fit(df)\n",
    "            #assign observations to group\n",
    "            output = model.transform(df)\n",
    "            #compute cost\n",
    "            eval_score = evaluator_sil.evaluate(output)\n",
    "            score.append(eval_score)\n",
    "\n",
    "        #plot the silhouette score\n",
    "        plt.plot(range(3,10), score)\n",
    "\n",
    "        #we choose K such that the silhouette score is maximized\n",
    "        #note that we have to add 3 as the index starts at 0 but possible values for K at 3\n",
    "        print(\"Based on the silhoutte score, the optimal number of clusters is \" + str(score.index(max(score))+3) +\".\")\n",
    "    except:\n",
    "        #troubleshooting\n",
    "        #this deals e.g. with the situation if a string or float element is given instead of a dataframe\n",
    "        print(\"Unexpected error:\", sys.exc_info()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df, K):    \n",
    "    try:\n",
    "        #get final clustering\n",
    "        kmeans = KMeans().setK(K).setSeed(123).setFeaturesCol(\"features_scaled\")\n",
    "        model = kmeans.fit(df)\n",
    "        df_clu = model.transform(df)\n",
    "        #rename  prediction column to label\n",
    "        df_clu = df_clu.withColumnRenamed(\"prediction\",\"label\")\n",
    "        return df_clu\n",
    "    except:\n",
    "        #troubleshooting\n",
    "        #this deals e.g. with the situation if a string or float element is given instead of a dataframe\n",
    "        print(\"Unexpected error:\", sys.exc_info()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree model is based on a tree-like structure. Each node represents one test. Depending on whether the test is successful or not, one of the child nodes is being tested. The final nodes of each path represent predictions. The following code was adapted from https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier and from https://github.com/Jcharis/pyspark-tutorials ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(trainingData, testData):\n",
    "    tokenizer = Tokenizer(inputCol = 'body',outputCol = 'body_toc') # converting the body into tokens\n",
    "    stopwords_remover = StopWordsRemover(inputCol = 'body_toc',outputCol = 'body_toc_filtered') # removing stopwords\n",
    "    vectorizer = CountVectorizer(inputCol = 'body_toc_filtered', outputCol = 'body_raw_features') # vectorising the words\n",
    "    idf = IDF(inputCol='body_raw_features', outputCol='body_vectorized_features') #getting the inverse document freqeuncy of the tokens\n",
    "    dt = DecisionTreeClassifier(featuresCol = 'body_vectorized_features', labelCol = 'label', maxDepth = 6) # we apply Decision tree classifier model to predict the label by body column\n",
    "    dt_pipeline = Pipeline(stages = (tokenizer,stopwords_remover,vectorizer,idf,dt)) # we create a pipeline for decision tree\n",
    "    \n",
    "    modeldt = dt_pipeline.fit(trainingData)#We train the model on the training data, which is 70% of the dataset, randomly chosen\n",
    "    predictions = modeldt.transform(testData)#We make predictions on the test data, which is 30% of the dataset, randomly chosen\n",
    "    #finally we only return relevant columns of the resulting dataframe\n",
    "    predictions = predictions.select(\"body\",\"num_comments\",\"score\",\"upvote_ratio\",\"features\",\"features_scaled\",\"prediction\", \"rawPrediction\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second machine learning model that is applied in this project is logistic regression. Logistic regression predicts the probability whether an observation belongs to a specific cluster or not. In general, logistic regression can only predict the probability if two clusters exists (as logistic regression eventually predicts True-or-False outputs given some probability threshold for True events). In our case there exist more than two clusters. Therefore, the logistic regression model has to be adapted, so the model can be used in a multiclass environment. In short, the algorithm predicts the probabilities whether a post belongs into each individual cluster. The cluster with the highest probability is then chosen as final prediction. For more details, please see the corresponding section in the report. The code was partly adapted from https://towardsdatascience.com/machine-learning-with-spark-f1dbc1363986 and https://github.com/Jcharis/pyspark-tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few codechunks are preparing the data for logistic regression and estimate the probability that a post belongs to each individual cluster. The next section defines a function that determines the final prediction based on the highest estimated probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####used to split the label up so it can be used for logistic regression\n",
    "###inputs\n",
    "##DF_train is the the clustered data frame\n",
    "## clusters is the number of clusters\n",
    "###outputs\n",
    "##DF_train is the clustered data frame prepared for logistic regression\n",
    "def createLabelingForLogistics(DF_train,clusters):\n",
    "    cluster_str = \"\" \n",
    "    empty_str = \"\" \n",
    "    for j in range(0,clusters): \n",
    "        cluster_str += str(j) #creates a string 123...  for which length = number of clusters\n",
    "        empty_str += '0' # creats a str with length = number of clusters and then fills that string with 0\n",
    "    for  i in range(0,clusters):\n",
    "        true_list = list(empty_str) \n",
    "        true_list[i] = '1' # adds a 1 to postion i in the true string so it can be correctly  put int the labels \n",
    "        true_str = ''.join(true_list)\n",
    "        DF_train = DF_train.withColumn(\"label\"+str(i), translate('label', cluster_str ,true_str ).cast('float'))  # uses the translate method to assign 1 or 0 to the label columns \n",
    "        #depending on its label \n",
    "    return DF_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####creates the pipeline for predicting\n",
    "###input:\n",
    "##label_name ... the name of the column that should be predicted\n",
    "###output:\n",
    "##lr_pipline the pipeline for the logistic regression\n",
    "def createPipeline(label_name):\n",
    "    tokenizer = Tokenizer(inputCol = 'body',outputCol = 'body_toc') # splitting the body into words\n",
    "    stopwords_remover = StopWordsRemover(inputCol = 'body_toc',outputCol = 'body_toc_filtered') # removing stopwords\n",
    "    vectorizer = CountVectorizer(inputCol = 'body_toc_filtered', outputCol = 'body_raw_features') # vectorising the words\n",
    "    idf = IDF(inputCol='body_raw_features', outputCol='body_vectorized_features') #getting the inverse document frequncy of the words\n",
    "    lr = LogisticRegression(featuresCol = 'body_vectorized_features', labelCol = label_name ) #train the logistic regression model\n",
    "    lr_pipeline = Pipeline(stages = (tokenizer,stopwords_remover,vectorizer,idf,lr))\n",
    "    return lr_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####takes the data and estimates the models and predicts the outcome for the test data\n",
    "###inputs:\n",
    "##trainDF = the training data\n",
    "##testDF = the testing data\n",
    "##clusters = the amount of clusters\n",
    "###outputs:\n",
    "## the test dataframe with the predictions\n",
    "\n",
    "def getModelPredictions(trainDF,testDF,clusters):\n",
    "    lr_models = []\n",
    "    columns = StructType([])\n",
    "    lr_predictions=[]\n",
    "    for i in range(0, clusters):\n",
    "        label_name=\"label\"+str(i) # getting the name of the label to predict\n",
    "        lr_pipeline = createPipeline(label_name) # making the pipeline\n",
    "        lr_models.append(lr_pipeline.fit(trainDF)) # getting the model\n",
    "\n",
    "        lr_predictions.append(lr_models[i].transform(testDF)) #getting the predictions\n",
    "        \n",
    "        #renaming the columns\n",
    "        lr_predictions[i] = lr_predictions[i].withColumnRenamed(\"RawPrediction\",\"RawPrediction\"+str(i))\n",
    "        lr_predictions[i] = lr_predictions[i].withColumnRenamed(\"probability\",\"probability\"+str(i))\n",
    "        lr_predictions[i] = lr_predictions[i].withColumnRenamed(\"prediction\",\"prediction\"+str(i))\n",
    "        if i > 0 :\n",
    "            lr_predictions[0] = lr_predictions[0].join(lr_predictions[i],lr_predictions[0].columns[0 : - 3 * i],'inner') # joining with the fist prediction so \n",
    "            #everything is in the same df\n",
    "    return lr_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the predictions into one final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done by choosing the category that was predicted with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####chooses the overall prediction for each post\n",
    "### inputs:\n",
    "## prediction_list ... the probabilities\n",
    "###output:\n",
    "## the predicted outcome\n",
    "\n",
    "def select_final_prediction_udf(prediction_list):\n",
    "    probability = 0\n",
    "    counter = 0\n",
    "    label = 0\n",
    "    for prediction in prediction_list:\n",
    "        if prediction[1] >= probability:\n",
    "            label = counter\n",
    "            probability = prediction[1]\n",
    "        counter+=1\n",
    "\n",
    "    return label\n",
    "    \n",
    "select_final_prediction = spark.udf.register(\"final_prediction\", select_final_prediction_udf) # registers select_final_orediction as a udf with spark, meaning that it can be used\n",
    "# to select the best prediction for each line in a dataframe\n",
    "\n",
    "####choses the overall prediction\n",
    "### inputs:\n",
    "## results ... the results of the separate logistic predictions as dataframe\n",
    "## testDF ... the test data as a dataframe\n",
    "###output: the final results of the lr as dataframe\n",
    "def make_final_prediction_column(results, testDF):\n",
    "    column_list = results.columns[len(testDF.columns)+4:-1] # get the list columns that were added by the prediction\n",
    "    selected_column_list = []\n",
    "    counter = -1\n",
    "    for i in column_list: # get the list of probability columns\n",
    "        if counter % 3 == 0:\n",
    "            selected_column_list.append(i)\n",
    "        counter+=1\n",
    "        \n",
    "    #use the list of probabilty columns to get the final probability\n",
    "    lr_results_df = results.withColumn(\"final_prediction\", select_final_prediction(array(selected_column_list)))\n",
    "    \n",
    "    return lr_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the final function of the logistic regression model\n",
    "#for applying the logistic regression model only this function has to be called\n",
    "#the functions above are helper functions that are called inside this final function.\n",
    "####run lr prediction\n",
    "###Inputs\n",
    "##trainDF ... training data\n",
    "##testDF ... testing Data\n",
    "##clusters ... number of clusters\n",
    "###output:\n",
    "## dataframe of test data with all the prediction outcomes\n",
    "def lr_prediction(trainDF, testDF, clusters):\n",
    "    trainDF_lr = createLabelingForLogistics(trainDF, clusters) #split the label to prepare the training data for logistic regression\n",
    "    results = getModelPredictions(trainDF_lr,testDF, clusters) # estimate the logistic regression model and get estimated probabilites\n",
    "    return make_final_prediction_column(results, testDF) #make the final predictions and return the test dataframe with the predicted clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the distance to assess model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As argued later in this notebook and in more detail in the report, we cannot rely on accuracy, or false or true positives because the collected data is missing a label. The label (i.e., the cluster) is determined by the algorithm. Therefore, the test data has no label assigned to it and many metrics cannot be used efficiently. Thus, we decided to compare the models obtained by logistic regression and decision trees based on the Euclidean distance between the true scaled values of the features (number of comments, score and upvote ratio) and the mean values of the corresponding predicted cluster. Finally, these distances are summed up. This measurement can be used to compare the efficiency of two or more models. The lower the distance measurement the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "####get the position means of the labels\n",
    "###inputs \n",
    "##clu ... the clustered data frame\n",
    "##K the number of clusters\n",
    "###output \n",
    "## a nested list containing the coordinates of the label means\n",
    "def get_labal_position_means(clu,K):\n",
    "    positions = []\n",
    "    for i in range(K): \n",
    "        features_scaled_df = clu.select(\"features_scaled\").where(clu.label == i)\n",
    "        features_scaled_df = features_scaled_df.withColumn(\"scaled\", vector_to_array(\"features_scaled\")).select([\"features_scaled\"] + [col(\"scaled\")[j] for j in range(3)]) #unpass the scaled features column\n",
    "        position = []\n",
    "        position.append(features_scaled_df.agg({'scaled[0]':'mean'}).select(\"avg(scaled[0])\").collect()[0][0]) #calculates the mean for num_comments\n",
    "        position.append(features_scaled_df.agg({'scaled[1]':'mean'}).select(\"avg(scaled[1])\").collect()[0][0])\n",
    "        position.append(features_scaled_df.agg({'scaled[2]':'mean'}).select(\"avg(scaled[2])\").collect()[0][0])\n",
    "        positions.append(position)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "####get the distance between the label means and the imput row\n",
    "###input \n",
    "##input ...  an array of columns containg the necessary position arguments\n",
    "###output\n",
    "##the distance between the label means and the input row\n",
    "def get_distance_udf(imput):\n",
    "    for i in range(len(imput)):\n",
    "        imput[i]=float(imput[i])\n",
    "    distance = math.sqrt((imput[0]-imput[3])**2+(imput[1]-imput[4])**2+(imput[2]-imput[5])**2)\n",
    "    return distance\n",
    "get_distance= spark.udf.register(\"distance\", get_distance_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "####makes a data frame that contains a distance measurement from the row to the label mean\n",
    "###inputs\n",
    "##clu ... clustered dataframe\n",
    "##results ... dataframe with the results of clustering\n",
    "##K ... number of clusters\n",
    "##prediction_column_name ...  the name of the column that gets predicted\n",
    "###output\n",
    "##a dataframe with the distance and other relevant results\n",
    "\n",
    "def make_distance_column(clu,results, K, prediction_column_name):\n",
    "    positions = get_labal_position_means(clu,K)\n",
    "    ####gets the first coordinate of the position of predicted cluster\n",
    "    ###inputs\n",
    "    ##label ... wich cluster has been predicted\n",
    "    ###output\n",
    "    ##the first coordinate of the predicted cluster\n",
    "    def get_position_0_udf(label):\n",
    "        return positions[label][0] #position is using the outer variable\n",
    "    get_position_0 = spark.udf.register(\"label_pos_0\", get_position_0_udf) # registering as udf for use with pyspark\n",
    "    ####gets the second coordinate of the position of predicted cluster\n",
    "    ###inputs\n",
    "    ##label ... which cluster has been predicted\n",
    "    ###output\n",
    "    ##the second coordinate of the predicted cluster\n",
    "    def get_position_1_udf(label):\n",
    "        return positions[label][1]\n",
    "    get_position_1 = spark.udf.register(\"label_pos_1\", get_position_1_udf)\n",
    "    ####gets the third coordinate of the position of predicted cluster\n",
    "    ###inputs\n",
    "    ##label ... which cluster has been predicted\n",
    "    ###output\n",
    "    ##the third coordinate of the predicted cluster\n",
    "    def get_position_2_udf(label):\n",
    "        return positions[label][2]    \n",
    "    get_position_2 = spark.udf.register(\"label_pos_2\", get_position_2_udf)\n",
    "    \n",
    "    results_lp = results.withColumn(\"label_pos_0\", get_position_0(col(prediction_column_name))) # get the first coordinate of the position of predicted cluster\n",
    "    results_lp = results_lp.withColumn(\"label_pos_1\", get_position_1(col(prediction_column_name))) # get the second coordinate of the position of predicted cluster\n",
    "    results_lp = results_lp.withColumn(\"label_pos_2\", get_position_2(col(prediction_column_name)))# get the third coordinate of the position of predicted cluster\n",
    "    results_lp = results_lp.withColumn(\"scaled\", vector_to_array(\"features_scaled\")).select([\"body\"] # prepare the right columns and devectorise features_scaled\n",
    "                                            + [\"num_comments\"] + [\"score\"] + [\"upvote_ratio\"] + [prediction_column_name]\n",
    "                                            + [\"label_pos_0\"] + [\"label_pos_1\"]+ [\"label_pos_2\"]+[\"features_scaled\"] + \n",
    "                                            [col(\"scaled\")[j] for j in range(3)]) \n",
    "    results_lp = results_lp.withColumn(\"distance\", get_distance(array(col(\"label_pos_0\"), col(\"label_pos_1\"), col(\"label_pos_2\"), col(\"scaled[0]\"),col(\"scaled[1]\"),col(\"scaled[2]\")))) # get the distances form the cluster mean\n",
    "    results_lp = results_lp.drop(\"label_pos_0\", \"label_pos_1\", \"label_pos_2\", \"scaled[0]\", \"scaled[1]\", \"scaled[2]\") # drop some unnecessary columns\n",
    "    return results_lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section analyzes the chosen subreddits, r/elderscrollsonline and r/Atlanta. Both subreddits might be relevant for marketing departments. Reddit posts could be used for promoting the Elder Scrolls Online video game or the city Atlanta. For such a promotion it is important to write a post that achieves as much attention (as upvotes and number of comments) as possible. This section explores whether it is possible to use logistic regression and decision trees to achieve predictions whether a post in the specific subreddit may get the required attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data\n",
    "In a first step we access the data from r/elderscrollsonline and r/Atlanta. The accessing-function also removes the youngest posts (we excluded all posts that were uploaded in the last 24 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eso = accessing(\"elderscrollsonline\",1000, 86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlanta = accessing(\"Atlanta\",1000, 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data locally\n",
    "The function above returns different data depending on the exact time (the data that is used in our analysis was collected on February 5th 2022 at approximately 22:00) the function was called. Therefore, to make results reproducible, we decided to add a csv-file of the respective data in our final submission. The next two code chunks save the dataframes locally. In order to easily save the data we are going to use pandas. While pandas itself is not scalable, this section is only included in order to make results more reproducible and all further steps will be done with a pyspark-dataframe (which again is scalable). The pandas-dataframes are only used to store the data locally and make results reproducible. Pandas should not be used for huge amount of data. Note that if the accessing functions above are called, there is no need to run this section. This section serves only to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eso.toPandas().to_csv('df_eso.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlanta.toPandas().to_csv(\"df_atlanta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next codechunks might be used to load the locally stored csv-files into a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eso = spark.read.csv('df_eso.csv', header=True, sep=\",\", inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlanta = spark.read.csv(\"df_atlanta.csv\", header=True, sep=\",\", inferSchema='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and preprocess the data\n",
    "Now, the collected data is cleaned and preprocessed. Some expressions are removed from the body. Finally, the explanatories are standardized so they can be used for clustering. After preprocessing the data, there are 754 observations about Elder Scrolls Online and 716 posts about Atlanta left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned dataframe consists of 754 observations.\n"
     ]
    }
   ],
   "source": [
    "df_eso = preprocessing(df_eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned dataframe consists of 716 observations.\n"
     ]
    }
   ],
   "source": [
    "df_atlanta = preprocessing(df_atlanta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the optimal number of clusters is determined. In the next step, the training data is clustered based on the optimal number of clusters determined by the maximum of the silhoutte score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "# note that for reproducibility we set the seed to 234\n",
    "(training_eso, test_eso) = df_eso.randomSplit([0.7, 0.3], 234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_atlanta, test_atlanta) = df_atlanta.randomSplit([0.7, 0.3], 234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the silhoutte score, the optimal number of clusters is 3.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_K(training_eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the silhoutte score, the optimal number of clusters is 8.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_K(training_atlanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_eso = 3\n",
    "clu_eso = cluster(training_eso, K_eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_atlanta = 8\n",
    "clu_atlanta = cluster(training_atlanta, K_atlanta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the clustered data. First we check how many posts are assigned to each individual cluster.\\\n",
    "For r/elderscrollsonline nearly all observations are assigned to cluster 0. This implies, that nearly all collected posts from r/elderscrollsonline are similar in terms of score, upvote ratio and number of comments. The ten comments that are not located in cluster 0 might be outliers. This certainly leads to bias. This can be seen too in a later section where this data is used for estimating the models.\\\n",
    "For r/Atlanta the clustering looks better. The data is grouped into eight different clusters. However, there are still some clusters (in fact cluster 2, 5 and 7) that only contain a handful of observations. In the case of r/Atlanta this could imply that these posts are outliers (in terms of the metadata). It could, however, also be the case that these results from the fact that the sample size is quite low. Both cases might lead to biased and inconsistent results of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    2|    9|\n",
      "|    0|  502|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clu_eso.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|   52|\n",
      "|    6|  154|\n",
      "|    3|  212|\n",
      "|    5|    1|\n",
      "|    4|   22|\n",
      "|    7|    2|\n",
      "|    2|    3|\n",
      "|    0|   49|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clu_atlanta.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering method applied above automatically labels the clusters with integers (ranging from 0 to the total number of clusters). Now, let us look at the cluster means (i.e., the mean values of the number of comments, score and upvote ratio of each cluster). Note that the following code was adapted from https://stackoverflow.com/questions/36251004/pyspark-aggregation-on-multiple-columns .\\\n",
    "For r/elderscrollsonline our presumption that the observations of cluster 1 and 2 are outliers is supported by the following results. The majority of posts has an average number of comments of 12.62, an average score of 5.02 and an upvote ratio of 72.5%. While the three clusters do not differ that much in terms of the upvote ratio there are huge differences regarding the number of comments and the score. The post of cluster 1 has a significnat higher number of comments and score than the average post in cluster 0. As the project aims at predicting popularity (which could also be seen as attention) the clusters of r/elderscollsonline could be sorted from least popular to most popular: cluster 0, cluster 2, cluster 1.\\\n",
    "r/Atlanta has a similar situatio as r/elderscollsonline. The majority of posts is located in clusters that have a relatively low number of comments and score. Five observations, however, have really high values. To remark, this might indicate the presence of outliers or that the sample size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+\n",
      "|label| avg(num_comments)|        avg(score)| avg(upvote_ratio)|\n",
      "+-----+------------------+------------------+------------------+\n",
      "|    1|             454.0|             478.0|              0.77|\n",
      "|    2|135.88888888888889|245.44444444444446|              0.88|\n",
      "|    0|12.615537848605578|5.0239043824701195|0.7254382470119528|\n",
      "+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group the clustered data by label and get the average number of comments, score and upvote ratio.\n",
    "#for r/elderscollsonline:\n",
    "clu_eso.groupBy(\"label\").agg(avg(\"num_comments\"), avg(\"score\"), avg(\"upvote_ratio\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+\n",
      "|label| avg(num_comments)|        avg(score)| avg(upvote_ratio)|\n",
      "+-----+------------------+------------------+------------------+\n",
      "|    1|16.115384615384617| 4.596153846153846|0.5384615384615383|\n",
      "|    6|14.551948051948052| 9.909090909090908|0.7481818181818181|\n",
      "|    3| 20.64622641509434| 23.67924528301887|0.8960377358490562|\n",
      "|    5|             182.0|             544.0|              0.94|\n",
      "|    4| 86.68181818181819|124.36363636363636|0.9131818181818182|\n",
      "|    7|             301.5|             325.5|0.9550000000000001|\n",
      "|    2|             306.0|             170.0|0.9533333333333333|\n",
      "|    0|131.20408163265307|  22.3265306122449|0.8981632653061222|\n",
      "+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for r/elderscollsonline:\n",
    "clu_atlanta.groupBy(\"label\").agg(avg(\"num_comments\"), avg(\"score\"), avg(\"upvote_ratio\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustered training data is now used to train a logistic regression model. Secondly, the model predicts the cluster of the observations in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "eso_results_lr = lr_prediction(clu_eso, test_eso, K_eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlanta_results_lr = lr_prediction(clu_atlanta, test_atlanta, K_atlanta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For r/elderscrollsonline most posts are predicted to lie in cluster 0. This confirms the argumentation in the section about clustering. Most training data is labeled as cluster 0. Therefore, one would suspect that the majority of the test data is predicted to be in cluster 0. There are not enough observations loacted in cluster 1 and 2 to consistently estimate a model. This result is not very appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|final_prediction|count|\n",
      "+----------------+-----+\n",
      "|               0|  241|\n",
      "|               2|    1|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eso_results_lr.groupBy(\"final_prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For r/Atlanta the predictions are more varied across the different clusters. Without running further model diagnostics, it seems that the logistic regression model (together with the collected data) is more useful for predicting the popularity of posts about Atlanta than for Elder Scrolls Online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|final_prediction|count|\n",
      "+----------------+-----+\n",
      "|               7|    6|\n",
      "|               3|   58|\n",
      "|               0|   25|\n",
      "|               5|   14|\n",
      "|               6|   26|\n",
      "|               1|   36|\n",
      "|               4|   48|\n",
      "|               2|    8|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atlanta_results_lr.groupBy(\"final_prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section applies the decision tree method. First, the model is trained and, secondly, the clusters of the posts in the test set are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eso_results_dt = getPrediction(clu_eso, test_eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlanta_results_dt = getPrediction(clu_atlanta, test_atlanta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For r/elderscrolls the decision tree model returns similar predictions as logistic regression. The majority of posts seems to be located in the most unpopular cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  240|\n",
      "|       2.0|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eso_results_dt.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree model returns interesting estimates for r/Atlanta. First of all, note that observations of the test data are predicted to lie in only five out of eight clusters. The missing clusters are 2,5 and 7. These are exactly the clusters that do not have much observations in the training set. Secondly, estimates obtained for r/Atlanta by Decision trees are different to the ones obtained by logistic regression. This will be discussed in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   25|\n",
      "|       1.0|    1|\n",
      "|       4.0|    1|\n",
      "|       3.0|   39|\n",
      "|       6.0|  155|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atlanta_results_dt.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, one would run several model diagnostics in order to assess whether a model predicts accurate or not. The problem is that most methods require to know the true label of the test data. This project collects the data without any label and assigns the label to the training set by clustering methods. This means that there is no efficient way to assign the true labels to the test data. Because of that, several methods such as accuracy, true and false positives or ROC cannot be applied. \\\n",
    "First, we will compare the predictions of the two models depending on the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r/elderscrollsonline\n",
    "y = 0\n",
    "for i in range(eso_results_dt.count()):\n",
    "    if (eso_results_dt.select(\"prediction\").collect()[i][0] == int(eso_results_lr.select(\"final_prediction\").collect()[i][0])):\n",
    "        y += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987603305785124"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y/eso_results_dt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r/Atlanta\n",
    "x = 0\n",
    "for i in range(atlanta_results_dt.count()):\n",
    "    if (atlanta_results_dt.select(\"prediction\").collect()[i][0] == int(atlanta_results_lr.select(\"final_prediction\").collect()[i][0])):\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2398190045248869"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/atlanta_results_dt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we will compare the accuracy of the two models based on the distance between the true values (the standardized values of number of comments, score and upvote ratio) and the mean values of the predicted clusters. Mathematically, this computes the distance for each observation (which can be seen in our case as a point in a three dimensional space) and its corresponding center mean. Secondly it sums up the distances. The distance itself does not give that much information. It can, however, be used to compare two or more models. The lower the sum of the distances the better are the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    sum(distance)|\n",
      "+-----------------+\n",
      "|273.4167650769202|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute the distance measurement for r/elderscrollsonline obtained by logistic regression\n",
    "is_eso_lr = make_distance_column(clu_eso, eso_results_lr, K_eso, \"final_prediction\")\n",
    "dis_eso_lr.agg({'distance': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     sum(distance)|\n",
      "+------------------+\n",
      "|290.43315429333927|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute the distance measurement for r/elderscrollsonline obtained by decision trees\n",
    "dis_eso_dt = make_distance_column(clu_eso, eso_results_dt.withColumn(\"prediction\", eso_results_dt.prediction.cast(\"int\")), K_eso, \"prediction\")\n",
    "dis_eso_dt.agg({'distance': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For r/elderscrollsonline the distance measurement is not that much different. This is supported by the fact that over 90% of all predictions are the same for both models. Both models estimate equally well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    sum(distance)|\n",
      "+-----------------+\n",
      "|671.0777058893595|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute the distance measurement for r/atlanta obtained by logistic regression\n",
    "dis_atlanta_lr = make_distance_column(clu_atlanta, atlanta_results_lr, K_atlanta, \"final_prediction\")\n",
    "dis_atlanta_lr.agg({'distance': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     sum(distance)|\n",
      "+------------------+\n",
      "|270.02247582905017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute the distance measurement for r/atlanta obtained by decision trees\n",
    "dis_atlanta_dt = make_distance_column(clu_atlanta, atlanta_results_dt.withColumn(\"prediction\", atlanta_results_dt.prediction.cast(\"int\")), K_atlanta, \"prediction\")\n",
    "dis_atlanta_dt.agg({'distance': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distance measurement computed for the test data from r/Atlanta the model obtained by decision trees is significantly more accurate and should give better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
